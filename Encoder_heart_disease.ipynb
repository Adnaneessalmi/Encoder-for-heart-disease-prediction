{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "NxgC1VXjyzVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ts8BzKFcwhVo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration class\n",
        "# Centralized configuration to easily manage hyperparameters, file paths, and other settings.\n"
      ],
      "metadata": {
        "id": "ifd4OCNry1E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "    MODEL_SAVE_PATH = \"/content/ft_transformer_model.pth\"\n",
        "    RANDOM_STATE = 42\n",
        "    TEST_SIZE = 0.2\n",
        "    BATCH_SIZE = 64\n",
        "    LEARNING_RATE = 0.001\n",
        "    NUM_EPOCHS = 500\n",
        "    INPUT_DIM = 13\n",
        "    NUM_CLASSES = 2"
      ],
      "metadata": {
        "id": "AHqwv0acy2_G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load, preprocess, and engineer features"
      ],
      "metadata": {
        "id": "UP3h8TMJy_vL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(url):\n",
        "    column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
        "                    'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
        "    df = pd.read_csv(url, names=column_names, na_values='?')\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df = df.dropna()\n",
        "    df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)\n",
        "    return df\n",
        "\n",
        "def engineer_features(df):\n",
        "    df['age_group'] = pd.cut(df['age'], bins=[0, 40, 60, 80, 100], labels=['young', 'middle', 'senior', 'elderly'])\n",
        "    df['age_thalach'] = df['age'] * df['thalach']\n",
        "    df['thalach_bin'] = pd.qcut(df['thalach'], q=4, labels=['low', 'medium-low', 'medium-high', 'high'])\n",
        "    df = pd.get_dummies(df, columns=['age_group', 'thalach_bin', 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])\n",
        "    return df\n",
        "\n",
        "def split_and_scale_data(df):\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=Config.TEST_SIZE, random_state=Config.RANDOM_STATE)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test"
      ],
      "metadata": {
        "id": "DBCO86XKzCqT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split and scale data"
      ],
      "metadata": {
        "id": "HJLZHHHCzIhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(url):\n",
        "    column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
        "                    'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
        "    df = pd.read_csv(url, names=column_names, na_values='?')\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df = df.dropna()\n",
        "    df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)\n",
        "    return df\n",
        "\n",
        "def engineer_features(df):\n",
        "    df['age_group'] = pd.cut(df['age'], bins=[0, 40, 60, 80, 100], labels=['young', 'middle', 'senior', 'elderly'])\n",
        "    df['age_thalach'] = df['age'] * df['thalach']\n",
        "    df['thalach_bin'] = pd.qcut(df['thalach'], q=4, labels=['low', 'medium-low', 'medium-high', 'high'])\n",
        "    df = pd.get_dummies(df, columns=['age_group', 'thalach_bin', 'sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'])\n",
        "    return df\n",
        "\n",
        "def split_and_scale_data(df):\n",
        "    X = df.drop('target', axis=1)\n",
        "    y = df['target']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=Config.TEST_SIZE, random_state=Config.RANDOM_STATE)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test"
      ],
      "metadata": {
        "id": "AsbwiEJpzPUX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the FTTransformer model and create a model function"
      ],
      "metadata": {
        "id": "eo2CLW7OzVmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FTTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, d_model=64, nhead=4, num_layers=3, dim_feedforward=256, dropout=0.1):\n",
        "        super(FTTransformer, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
        "                                                   nhead=nhead,\n",
        "                                                   dim_feedforward=dim_feedforward,\n",
        "                                                   dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x).unsqueeze(1)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "def create_model():\n",
        "    return FTTransformer(input_dim=Config.INPUT_DIM, num_classes=Config.NUM_CLASSES)"
      ],
      "metadata": {
        "id": "bMZpgr9fzYUE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "zXLX357Qz6cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(X_train, y_train):\n",
        "    model = create_model()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
        "\n",
        "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{Config.NUM_EPOCHS}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), Config.MODEL_SAVE_PATH)\n",
        "    return model"
      ],
      "metadata": {
        "id": "P9hsbGy9z6rj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "ZpQAu72B0LLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(torch.FloatTensor(X_test))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    predicted = predicted.numpy()\n",
        "\n",
        "    accuracy = accuracy_score(y_test, predicted)\n",
        "    report = classification_report(y_test, predicted)\n",
        "    conf_matrix = confusion_matrix(y_test, predicted)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "\n",
        "    return accuracy, report, conf_matrix"
      ],
      "metadata": {
        "id": "iXHLheLi0R8D"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction functions"
      ],
      "metadata": {
        "id": "GXZxRcii0eMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, X_new):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(torch.FloatTensor(X_new))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    return predicted.numpy()\n",
        "\n",
        "def predict_proba(model, X_new):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(torch.FloatTensor(X_new))\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "    return probabilities.numpy()"
      ],
      "metadata": {
        "id": "N2QCAjyR0emH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot confusion matrix"
      ],
      "metadata": {
        "id": "pa1AiCKY0lA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(conf_matrix):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "z3QJUGVc0ldb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main function to orchestrate the workflow"
      ],
      "metadata": {
        "id": "ee2-H6B30r0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Load and preprocess data\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    df = load_data(Config.DATA_URL)\n",
        "    df = preprocess_data(df)\n",
        "    df = engineer_features(df)\n",
        "\n",
        "    # Split and scale data\n",
        "    print(\"Splitting and scaling data...\")\n",
        "    X_train_scaled, X_test_scaled, y_train, y_test = split_and_scale_data(df)\n",
        "\n",
        "    # Update input dimension based on engineered features\n",
        "    Config.INPUT_DIM = X_train_scaled.shape[1]\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    model = train_model(X_train_scaled, y_train)\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"Evaluating model...\")\n",
        "    accuracy, report, conf_matrix = evaluate_model(model, X_test_scaled, y_test)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(conf_matrix)\n",
        "\n",
        "    # Example prediction\n",
        "    print(\"Making example predictions...\")\n",
        "    new_data = X_test_scaled[:5]  # Just using first 5 test samples as an example\n",
        "    predictions = predict(model, new_data)\n",
        "    probabilities = predict_proba(model, new_data)\n",
        "\n",
        "    print(\"Example predictions:\", predictions)\n",
        "    print(\"Prediction probabilities:\")\n",
        "    for i, prob in enumerate(probabilities):\n",
        "        print(f\"Sample {i+1}: Class 0: {prob[0]:.4f}, Class 1: {prob[1]:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "iDr9oBWv0x-g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}